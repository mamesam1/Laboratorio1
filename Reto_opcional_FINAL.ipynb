{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamesam1/Laboratorio1/blob/main/Reto_opcional_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** RETO (opcional): Implementación algoritmo de descenso de gradiente en problema con múltiples minínimos locales **\n",
        "**Facultad de ingeniería, departamento de Ingeniería Biomédica, Universidad de los Andes**\\\n",
        "**IBIO-2440 Programación científica**"
      ],
      "metadata": {
        "id": "775dekHVI9gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nombres de los integrantes**\n",
        "\n",
        "\n",
        "1.   Nombre integrante 1\n",
        "2.   Nombre integrante 2\n",
        "\n",
        "**Número del grupo**\n",
        "\n",
        "*Escribir el número del grupo*"
      ],
      "metadata": {
        "id": "FHIOLOzzJK2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considere el problema de optimización:\n",
        "\n",
        "\n",
        "\n",
        "> $\\min_x f(x)$ sujeto a $x\\in\\mathbb{R}^3$\n",
        "\n",
        "donde $f:\\mathbb{R^3}→\\mathbb{R}$ y $x=[x_1,x_2,x_3]^T$. Note que este es un problema de minmización sin restricciones en $\\mathbb{R}^3$. Se sabe que la función es derivable y posee varios minimizadores locales entre los cuales hayun minimizador global, todos con posiciones desconocidas. El objetivo del presente laboratorio bono es implementar y utilizar el algoritmo de descenso de gradiente para identificar la mayor cantidad de candidatos a minimizadores locales dentro de una región dada e identificar el posible minimizador global. \n",
        "\n",
        "*   Implemente su propia rutina del algoritmo de descenso de gradiente teniendo en cuenta que los minimizadores de la función están definida por $x_i ∈ [-5, 5]$ para cada $i = 1, …, 3$. Es decir, cada componente de $x$ està entre $-5$ y $5$. Defina un tamaño del salto $\\alpha$ y un parámetro de criterio de parada $ϵ$ que usted considere conveniente para resolver el problema. Estime el gradiente de la función evaluado en un punto de forma numérica (como se hizo en una práctica anterior), y bajo ninguna circunstancia utilice variables simbólicas.\n",
        "   \n",
        "*    Corra el algoritmo de descenso de gradiente al menos para 500 condiciones iniciales diferentes, y guarde las soluciones resultantes de cada corrida (ya sea definidas de forma aleatoria o haciendo un recorrido definido en la región donde se sabe están los minimizadores locales). Basados en estos resultados, realice una tabla donde incluya los valores aproximados de los candidatos a minimizadores locales (recuerden que cada candidato es un vector 3-dimensional), el candidato a minimizador global, y el respectivo número de iteraciones que el algoritmo de descenso de gradiente requirió para llegar a la respuesta.\n",
        "\n",
        "\\\\\n",
        "\n",
        "Esta actividad funciona como un bono para la nota del primer parcial de los integrantes del grupo. La nota se asignará dependiendo de el correcto seguimiento de las instrucciones anteriores y la cantidad de minimizadores locales reales que su algoritmo encuentre. Cada grupo debe desarrollar su propia solución y no se permitirá copia entre grupos de todo el curso. Una falta ante esto se reportará inmediatamente ante el comité de ética de la facultad. \n",
        "\n"
      ],
      "metadata": {
        "id": "WqexwZZjJUFQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2XK-Ktgm8D-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0270750a-2fe9-4076-f318-dbb005425ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: benchmark_functions in /usr/local/lib/python3.8/dist-packages (1.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from benchmark_functions) (23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from benchmark_functions) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "#@title Corra esta celda para cargar los datos del ejercicio\n",
        "!pip install benchmark_functions\n",
        "import benchmark_functions as bf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# func es el objeto creado con la funcion a minimizar\n",
        "func = bf.StyblinskiTang(n_dimensions=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dado un punto x0, func permite evaluar la función objetivo en x0 \n",
        "x0 = np.array([1.0, 1.0, 1.0]) \n",
        "f=func(x0)\n",
        "\n",
        "#Algoritmo\n",
        "def descensoGradiente(punto_inicial, epsilon, alpha, ite):\n",
        "  x = punto_inicial\n",
        "  for k in range(1, ite + 1):\n",
        "    #Gradiente\n",
        "    hx = np.array([x[0] + 0.1, x[1], x[2]])\n",
        "    hy = np.array([x[0], x[1] + 0.1, x[2]])\n",
        "    hz = np.array([x[0], x[1], x[2] + 0.1])\n",
        "\n",
        "    gradx = (func(hx) - func(x))/0.1\n",
        "    grady = (func(hy) - func(x))/0.1\n",
        "    gradz = (func(hz) - func(x))/0.1\n",
        "    grad = np.array([gradx, grady, gradz])\n",
        "    #Dirección de movimiento\n",
        "    direc = -alpha*grad   \n",
        "\n",
        "    #Iteración\n",
        "    x = np.add(x, direc)\n",
        "\n",
        "    #Criterio de parada\n",
        "\n",
        "    if np.linalg.norm(grad) <= epsilon:\n",
        "      return x\n",
        "      break\n",
        "\n",
        "    elif k == ite and np.linalg.norm(grad) >= epsilon:\n",
        "      return \"No converge\"\n"
      ],
      "metadata": {
        "id": "YWsqQ0tq9D84"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def minimoGlobal():\n",
        "\n",
        "  #Los minimos que vayamos encontrando, los metemos a un array para compararlos luego\n",
        "  minimosCandidatos = np.array([])\n",
        "  minimizadoresCandidatos = np.array([])\n",
        "\n",
        "  epsilon = 0.01\n",
        "  alpha = 0.01\n",
        "  ite = 100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Hacemos descenso de gradiente 5 veces con puntos iniciales diferentes\n",
        "  for x in range(0,500,1):\n",
        "\n",
        "\n",
        "    #Punto inicial ... x1i x2i x3i \n",
        "    X1i = float(np.random.randint(-5,5))\n",
        "    X2i = float(np.random.randint(-5,5))\n",
        "    X3i = float(np.random.randint(-5,5))\n",
        "    \n",
        "    puntoInicial = np.array([X1i,X2i,X3i])\n",
        "  \n",
        "\n",
        "    #Punto final ... x1f x2f x3f\n",
        "    \n",
        "    \n",
        "    \n",
        "    minimizadorEncontrado = descensoGradiente(puntoInicial, epsilon, alpha, ite)\n",
        "    \n",
        "    if(minimizadorEncontrado != 'No converge'):\n",
        "      minimizadoresCandidatos  = np.append(minimizadoresCandidatos, minimizadorEncontrado)\n",
        "      minimosCandidatos = np.append(minimosCandidatos, func(minimizadorEncontrado)) \n",
        "\n",
        "  #print(minimosCandidatos)\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  plt.hist(minimosCandidatos)\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "minimoGlobal()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "-w9H0-0e-peK",
        "outputId": "6467686a-d2f1-41d2-9b93-06c483ee3709"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-d244bc2b766b>:32: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if(minimizadorEncontrado != 'No converge'):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}